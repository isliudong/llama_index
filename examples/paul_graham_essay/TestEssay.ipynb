{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f1a9eb90-335c-4214-8bb6-fd1edbe3ccbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# My OpenAI Key\n",
    "import os\n",
    "os.environ['OPENAI_API_KEY'] = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6a712b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be3f7baa-1c0a-430b-981b-83ddca9e71f2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Using GPT Tree Index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0881f151-279e-4910-95c7-f49d3d6a4c69",
   "metadata": {},
   "source": [
    "#### [Demo] Default leaf traversal "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8d0b2364-4806-4656-81e7-3f6e4b910b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import GPTTreeIndex, SimpleDirectoryReader\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1c297fd3-3424-41d8-9d0d-25fe6310ab62",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "documents = SimpleDirectoryReader('data').load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "370fd08f-56ff-4c24-b0c4-c93116a6d482",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.indices.common_tree.base:> Building index from nodes: 2 chunks\n",
      "> Building index from nodes: 2 chunks\n",
      "> Building index from nodes: 2 chunks\n",
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total LLM token usage: 9890 tokens\n",
      "> [build_index_from_nodes] Total LLM token usage: 9890 tokens\n",
      "> [build_index_from_nodes] Total LLM token usage: 9890 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total embedding token usage: 0 tokens\n",
      "> [build_index_from_nodes] Total embedding token usage: 0 tokens\n",
      "> [build_index_from_nodes] Total embedding token usage: 0 tokens\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"http_proxy\"] = \"http://127.0.0.1:10809\"\n",
    "os.environ[\"https_proxy\"] = \"http://127.0.0.1:10809\"\n",
    "new_index = GPTTreeIndex.from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bd14686d-1c53-4637-9340-3745f2121ae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.indices.tree.select_leaf_retriever:>[Level 0] Selected node: [2]/[2]\n",
      ">[Level 0] Selected node: [2]/[2]\n",
      ">[Level 0] Selected node: [2]/[2]\n",
      "INFO:llama_index.indices.tree.select_leaf_retriever:>[Level 1] Selected node: [5]/[5]\n",
      ">[Level 1] Selected node: [5]/[5]\n",
      ">[Level 1] Selected node: [5]/[5]\n",
      "INFO:llama_index.token_counter.token_counter:> [retrieve] Total LLM token usage: 4122 tokens\n",
      "> [retrieve] Total LLM token usage: 4122 tokens\n",
      "> [retrieve] Total LLM token usage: 4122 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [retrieve] Total embedding token usage: 0 tokens\n",
      "> [retrieve] Total embedding token usage: 0 tokens\n",
      "> [retrieve] Total embedding token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total LLM token usage: 985 tokens\n",
      "> [get_response] Total LLM token usage: 985 tokens\n",
      "> [get_response] Total LLM token usage: 985 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total embedding token usage: 0 tokens\n",
      "> [get_response] Total embedding token usage: 0 tokens\n",
      "> [get_response] Total embedding token usage: 0 tokens\n"
     ]
    }
   ],
   "source": [
    "# set Logging to DEBUG for more detailed outputs\n",
    "query_engine = new_index.as_query_engine()\n",
    "response = query_engine.query(\"What did the author do growing up?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b4c87d14-d2d8-4d80-89f6-1e5972973528",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "<b>\nThe author grew up working on things that weren't prestigious, such as still life painting, starting Viaweb and Y Combinator, writing essays, working on spam filters, painting, cooking for groups, buying a building in Cambridge, writing essays, giving talks, and scheming with Robert and Trevor about projects.</b>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(f\"<b>{response}</b>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "68c9ebfe-b1b6-4f4e-9278-174346de8c90",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.indices.tree.select_leaf_retriever:>[Level 0] Selected node: [3]/[3]\n",
      ">[Level 0] Selected node: [3]/[3]\n",
      ">[Level 0] Selected node: [3]/[3]\n",
      "INFO:llama_index.token_counter.token_counter:> [retrieve] Total LLM token usage: 3447 tokens\n",
      "> [retrieve] Total LLM token usage: 3447 tokens\n",
      "> [retrieve] Total LLM token usage: 3447 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [retrieve] Total embedding token usage: 0 tokens\n",
      "> [retrieve] Total embedding token usage: 0 tokens\n",
      "> [retrieve] Total embedding token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total LLM token usage: 0 tokens\n",
      "> [get_response] Total LLM token usage: 0 tokens\n",
      "> [get_response] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total embedding token usage: 0 tokens\n",
      "> [get_response] Total embedding token usage: 0 tokens\n",
      "> [get_response] Total embedding token usage: 0 tokens\n"
     ]
    }
   ],
   "source": [
    "# set Logging to DEBUG for more detailed outputs\n",
    "response = query_engine.query(\"What did the author do after his time at Y Combinator?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a5ab5943-7c84-4c2b-ac99-ec4b5fc67e64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "<b>None</b>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(f\"<b>{response}</b>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c62ec3-c3cf-467e-ab0f-88ffb9f990be",
   "metadata": {},
   "source": [
    "#### [Demo] Leaf traversal with child_branch_factor=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "46714db4-9592-4c55-9ca7-916758f2ce68",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.indices.tree.select_leaf_retriever:>[Level 0] Selected node: [1]/[1,2]\n",
      ">[Level 0] Selected node: [1]/[1,2]\n",
      ">[Level 0] Selected node: [1]/[1,2]\n",
      "INFO:llama_index.indices.tree.select_leaf_retriever:>[Level 0] Selected node: [2]/[1,2]\n",
      ">[Level 0] Selected node: [2]/[1,2]\n",
      ">[Level 0] Selected node: [2]/[1,2]\n",
      "INFO:llama_index.indices.tree.select_leaf_retriever:>[Level 1] Selected node: [1]/[1,5]\n",
      ">[Level 1] Selected node: [1]/[1,5]\n",
      ">[Level 1] Selected node: [1]/[1,5]\n",
      "INFO:llama_index.indices.tree.select_leaf_retriever:>[Level 1] Selected node: [5]/[1,5]\n",
      ">[Level 1] Selected node: [5]/[1,5]\n",
      ">[Level 1] Selected node: [5]/[1,5]\n",
      "INFO:llama_index.token_counter.token_counter:> [retrieve] Total LLM token usage: 4207 tokens\n",
      "> [retrieve] Total LLM token usage: 4207 tokens\n",
      "> [retrieve] Total LLM token usage: 4207 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [retrieve] Total embedding token usage: 0 tokens\n",
      "> [retrieve] Total embedding token usage: 0 tokens\n",
      "> [retrieve] Total embedding token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total LLM token usage: 1932 tokens\n",
      "> [get_response] Total LLM token usage: 1932 tokens\n",
      "> [get_response] Total LLM token usage: 1932 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total embedding token usage: 0 tokens\n",
      "> [get_response] Total embedding token usage: 0 tokens\n",
      "> [get_response] Total embedding token usage: 0 tokens\n"
     ]
    }
   ],
   "source": [
    "# try using branching factor 2\n",
    "query_engine = new_index.as_query_engine(\n",
    "    child_branch_factor=2\n",
    ")\n",
    "response = query_engine.query(\"What did the author do growing up?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1ea7f891-b7e1-497a-a965-14201b220404",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "<b>\nGrowing up, the author wrote short stories, programmed on an IBM 1401, and eventually convinced his father to buy him a TRS-80 microcomputer. With the microcomputer, he wrote simple games, a program to predict how high his model rockets would fly, and a word processor. He also studied philosophy in college, but eventually switched to AI. He also studied painting and drawing at RISD, took an entrance exam to study art in Florence, and painted still lives in his bedroom at night.</b>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(f\"<b>{response}</b>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c572726-bb95-49c3-a762-d966de59ee5f",
   "metadata": {},
   "source": [
    "#### [Demo] Build Tree Index during Query-Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "255fb052-1ff6-4f27-881f-28d4790e9520",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = SimpleDirectoryReader('data').load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "85371256-292c-473e-9485-7de5c1997a59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total embedding token usage: 0 tokens\n",
      "> [build_index_from_nodes] Total embedding token usage: 0 tokens\n",
      "> [build_index_from_nodes] Total embedding token usage: 0 tokens\n"
     ]
    }
   ],
   "source": [
    "index_light = GPTTreeIndex.from_documents(documents, build_tree=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "77b0acb3-5593-4f00-8eef-315a031fedc2",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "TreeAllLeafRetriever.__init__() got an unexpected keyword argument 'response_mode'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[31], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m query_engine \u001B[38;5;241m=\u001B[39m \u001B[43mindex_light\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mas_query_engine\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m      2\u001B[0m \u001B[43m    \u001B[49m\u001B[43mretriever_mode\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mall_leaf\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m      3\u001B[0m \u001B[43m    \u001B[49m\u001B[43mresponse_mode\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mtree_summarize\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m      4\u001B[0m \u001B[43m)\u001B[49m\n\u001B[0;32m      5\u001B[0m query_engine\u001B[38;5;241m.\u001B[39mquery(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mWhat did the author do after his time at Y Combinator?\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32mD:\\PycharmProjects\\llama_index\\llama_index\\indices\\base.py:241\u001B[0m, in \u001B[0;36mBaseGPTIndex.as_query_engine\u001B[1;34m(self, **kwargs)\u001B[0m\n\u001B[0;32m    237\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mas_query_engine\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m BaseQueryEngine:\n\u001B[0;32m    238\u001B[0m     \u001B[38;5;66;03m# NOTE: lazy import\u001B[39;00m\n\u001B[0;32m    239\u001B[0m     \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mllama_index\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mquery_engine\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mretriever_query_engine\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m RetrieverQueryEngine\n\u001B[1;32m--> 241\u001B[0m     retriever \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mas_retriever(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    243\u001B[0m     kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mretriever\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m retriever\n\u001B[0;32m    244\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mservice_context\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m kwargs:\n",
      "File \u001B[1;32mD:\\PycharmProjects\\llama_index\\llama_index\\indices\\tree\\base.py:108\u001B[0m, in \u001B[0;36mGPTTreeIndex.as_retriever\u001B[1;34m(self, retriever_mode, **kwargs)\u001B[0m\n\u001B[0;32m    106\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m TreeRootRetriever(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    107\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m retriever_mode \u001B[38;5;241m==\u001B[39m TreeRetrieverMode\u001B[38;5;241m.\u001B[39mALL_LEAF:\n\u001B[1;32m--> 108\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m TreeAllLeafRetriever(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    109\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    110\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUnknown retriever mode: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mretriever_mode\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[1;31mTypeError\u001B[0m: TreeAllLeafRetriever.__init__() got an unexpected keyword argument 'response_mode'"
     ]
    }
   ],
   "source": [
    "query_engine = index_light.as_query_engine(\n",
    "    retriever_mode=\"all_leaf\",\n",
    "    response_mode='tree_summarize',\n",
    ")\n",
    "query_engine.query(\"What did the author do after his time at Y Combinator?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9773497-9aa6-4a16-884a-cd882e63d012",
   "metadata": {},
   "source": [
    "#### [Demo] Build Tree Index with a custom Summary Prompt, directly retrieve answer from root node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8ab6d3ad-95e1-477a-a0dc-2ce4763ff2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import SummaryPrompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5a91a445-6ab2-457c-850e-79c5386129db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.indices.common_tree.base:> Building index from nodes: 2 chunks\n",
      "> Building index from nodes: 2 chunks\n",
      "> Building index from nodes: 2 chunks\n",
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total LLM token usage: 9639 tokens\n",
      "> [build_index_from_nodes] Total LLM token usage: 9639 tokens\n",
      "> [build_index_from_nodes] Total LLM token usage: 9639 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total embedding token usage: 0 tokens\n",
      "> [build_index_from_nodes] Total embedding token usage: 0 tokens\n",
      "> [build_index_from_nodes] Total embedding token usage: 0 tokens\n"
     ]
    }
   ],
   "source": [
    "documents = SimpleDirectoryReader('data').load_data()\n",
    "\n",
    "query_str = \"What did the author do growing up?\"\n",
    "SUMMARY_PROMPT_TMPL = (\n",
    "    \"Context information is below. \\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"{context_str}\"\n",
    "    \"\\n---------------------\\n\"\n",
    "    \"Given the context information and not prior knowledge, \"\n",
    "    f\"answer the question: {query_str}\\n\"\n",
    ")\n",
    "SUMMARY_PROMPT = SummaryPrompt(SUMMARY_PROMPT_TMPL)\n",
    "index_with_query = GPTTreeIndex.from_documents(documents, summary_template=SUMMARY_PROMPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9223ffa8-d49d-4de3-821a-701b2a0352d4",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'GPTTreeIndex' object has no attribute 'query'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[34], line 5\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# directly retrieve response from root nodes instead of traversing tree\u001B[39;00m\n\u001B[0;32m      2\u001B[0m query_engine \u001B[38;5;241m=\u001B[39m index_with_query\u001B[38;5;241m.\u001B[39mas_query_engine(\n\u001B[0;32m      3\u001B[0m     retriever_mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mroot\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m      4\u001B[0m )\n\u001B[1;32m----> 5\u001B[0m response \u001B[38;5;241m=\u001B[39m \u001B[43mindex_with_query\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mquery\u001B[49m(query_str)\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'GPTTreeIndex' object has no attribute 'query'"
     ]
    }
   ],
   "source": [
    "# directly retrieve response from root nodes instead of traversing tree\n",
    "query_engine = index_with_query.as_query_engine(\n",
    "    retriever_mode=\"root\"\n",
    ")\n",
    "response = index_with_query.query(query_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fdca6970-2f3f-4741-ae98-555db8d3d9a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "<b>\nGrowing up, the author wrote short stories, programmed on an IBM 1401, and eventually convinced his father to buy him a TRS-80 microcomputer. With the microcomputer, he wrote simple games, a program to predict how high his model rockets would fly, and a word processor. He also studied philosophy in college, but eventually switched to AI. He also studied painting and drawing at RISD, took an entrance exam to study art in Florence, and painted still lives in his bedroom at night.</b>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(f\"<b>{response}</b>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6457769-dfaf-4241-ab32-dcf901dde902",
   "metadata": {},
   "source": [
    "## Using GPT Keyword Table Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d59ef6-70b0-47bb-818d-7237a3b7de75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import GPTKeywordTableIndex, SimpleDirectoryReader\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3f1c67-6d73-4f37-afcf-9e637002fcff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build keyword index\n",
    "documents = SimpleDirectoryReader('data').load_data()\n",
    "index = GPTKeywordTableIndex.from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d4f686-6825-49cf-a113-d2fdd484de77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set Logging to DEBUG for more detailed outputs\n",
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"What did the author do after his time at Y Combinator?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a483514d-4ab5-489d-8b99-7250df491ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(f\"<b>{response}</b>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae1bea9-b534-430a-a52b-1f4414957ac9",
   "metadata": {},
   "source": [
    "## Using GPT List Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1aa8c8c1-7fce-4737-9141-d14fd37a779c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import GPTListIndex, SimpleDirectoryReader\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "191caa65-a77f-4d8c-b095-4aed61300ea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total embedding token usage: 0 tokens\n",
      "> [build_index_from_nodes] Total embedding token usage: 0 tokens\n",
      "> [build_index_from_nodes] Total embedding token usage: 0 tokens\n"
     ]
    }
   ],
   "source": [
    "# build list index\n",
    "documents = SimpleDirectoryReader('data').load_data()\n",
    "index = GPTListIndex.from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1b3d4bd8-7540-4c6f-8616-ab2d8c6ae2b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total LLM token usage: 28940 tokens\n",
      "> [get_response] Total LLM token usage: 28940 tokens\n",
      "> [get_response] Total LLM token usage: 28940 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total embedding token usage: 0 tokens\n",
      "> [get_response] Total embedding token usage: 0 tokens\n",
      "> [get_response] Total embedding token usage: 0 tokens\n"
     ]
    }
   ],
   "source": [
    "# set Logging to DEBUG for more detailed outputs\n",
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"What did the author do after his time at Y Combinator?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5101b979-175f-490e-9b32-27689fe4b789",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(f\"<b>{response}</b>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65cfce56-853e-431b-888e-946771c3b07e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
